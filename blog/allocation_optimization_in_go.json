{"date":"2020-9-15","title":"Optimizing PGX Allocations in Golang with Pprof.","desc":"An example utilizing pprof for some impressive allocation reductions in the PGX sql library.","file":"allocation_optimization_in_go","html":"<h1>Optimizing PGX Allocations in Golang with Pprof.</h1>\n<p>Performance tuning is one of those programming rituals that gets oddly addicting.\nSeems like humans have a fundamental impulse to make a graph plot in their desired direction.\nThis can be seen in a wide assortment of fields.\nDay traders watch metrics focused on their net earnings, nutritionists keep their calorie counts logged, and programmers focusing on performance obsess over memory allocations.</p>\n<p>After spending sometime obessing myself I found myself making large allocation improvements with some tricks in the popular <a href=\"https://github.com/jackc/pgx\">PGX</a> library.</p>\n<p>I'd like to shout out <em>Kale Blanekship</em> and <em>Eric Chlebek</em> from the performance channel in #gophers slack. They provided the clues used in this post.</p>\n<h2>The code</h2>\n<p>The code that's being profiled is a new distributed lock implementation for <a href=\"https://github.com/quay/claircore/\">ClairCore</a>.\nPostgres is the only required infrastructure for ClairCore by design.\nWhile it's not the best mechanim for a distributed lock, <a href=\"https://www.postgresql.org/docs/9.1/explicit-locking.html\">postgres advisory locks</a> can be utilized to get <em>mostly</em> there.</p>\n<p>You can view the distlock implementation <a href=\"https://github.com/ldelossa/distlock\">here</a></p>\n<h2>Reducing channel allocations</h2>\n<p>Our distlock implementation utilizes the request/response channel-of-channel pattern.\nA request object with a response channel is pushed onto a request channel.\nWhen the receiver gets the request it writes to the response channel, unblocking any client listening.</p>\n<p>This pattern is useful but will also alloc a lot of channels resulting in bloating the heap.</p>\n<p>To demonstrate this a benchmark will be taken that profiles lock acquisition and lock return.</p>\n<pre class=\"hljs\"><code><span class=\"hljs-meta\">$</span><span class=\"bash\"> go <span class=\"hljs-built_in\">test</span> -benchtime <span class=\"hljs-string\">&quot;1m&quot;</span>  -run xxx -bench . -memprofile memprofile.out -cpuprofile cpuprofile.out</span>\n</code></pre>\n<p>The command above runs a 1 minute benchmark profiling both memory and cpu.</p>\n<p>Next lets start an interactive pprof session over the memory profile and drill into the function where the channel allocations are occuring.</p>\n<pre class=\"hljs\"><code><span class=\"hljs-meta\">$</span><span class=\"bash\"> go tool pprof distlock.test memprofile.out</span>\n\n(pprof) list \\.Lock\nTotal: 194.36MB\nROUTINE ======================== github.com/ldelossa/distlock.(*Manager).Lock in /home/louis/git/go/distlock/manager.go\n      20MB       20MB (flat, cum) 10.29% of Total\n         .          .     78:\t}\n         .          .     79:\n         .          .     80:\treq := request{\n         .          .     81:\t\tt:        Lock,\n         .          .     82:\t\tkey:      key,\n   13.50MB    13.50MB     83:\t\trespChan: make(chan response),\n         .          .     84:\t}\n         .          .     85:\n         .          .     86:\t// guaranteed to return\n         .          .     87:\tresp := m.g.request(req)\n         .          .     88:\n         .          .     89:\tif !resp.ok {\n         .          .     90:\t\treturn resp.ctx, func() {}\n         .          .     91:\t}\n         .          .     92:\n         .          .     93:\tm.propagateCancel(ctx, resp.ctx, key)\n         .          .     94:\n    6.50MB     6.50MB     95:\treturn resp.ctx, func() {\n         .          .     96:\t\tm.unlock(key)\n         .          .     97:\t}\n         .          .     98:}\n         .          .     99:\n         .          .    100:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {\n</code></pre>\n<p>Above illustrates 13.50MB of heap memory is spent on allocating request objects and their response channels.</p>\n<p>We can introduce an object pool to promote the reuse of these channels.</p>\n<pre class=\"hljs\"><code><span class=\"hljs-keyword\">type</span> reqPool <span class=\"hljs-keyword\">struct</span> {\n\tc <span class=\"hljs-keyword\">chan</span> request\n}\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">func</span> <span class=\"hljs-title\">NewReqPool</span><span class=\"hljs-params\">(seed <span class=\"hljs-keyword\">int</span>)</span> *<span class=\"hljs-title\">reqPool</span></span> {\n\tc := <span class=\"hljs-built_in\">make</span>(<span class=\"hljs-keyword\">chan</span> request, seed*<span class=\"hljs-number\">2</span>)\n\t<span class=\"hljs-keyword\">for</span> i := <span class=\"hljs-number\">0</span>; i &lt; seed; i++ {\n\t\tr := request{respChan: <span class=\"hljs-built_in\">make</span>(<span class=\"hljs-keyword\">chan</span> response)}\n\t\t<span class=\"hljs-keyword\">select</span> {\n\t\t<span class=\"hljs-keyword\">case</span> c &lt;- r:\n\t\t<span class=\"hljs-keyword\">default</span>:\n\n\t\t}\n\t}\n\t<span class=\"hljs-keyword\">return</span> &amp;reqPool{c}\n}\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">func</span> <span class=\"hljs-params\">(p *reqPool)</span> <span class=\"hljs-title\">Get</span><span class=\"hljs-params\">()</span> <span class=\"hljs-title\">request</span></span> {\n\t<span class=\"hljs-keyword\">select</span> {\n\t<span class=\"hljs-keyword\">case</span> r := &lt;-p.c:\n\t\t<span class=\"hljs-keyword\">return</span> r\n\t<span class=\"hljs-keyword\">default</span>:\n\t\t<span class=\"hljs-keyword\">return</span> request{respChan: <span class=\"hljs-built_in\">make</span>(<span class=\"hljs-keyword\">chan</span> response)}\n\t}\n}\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">func</span> <span class=\"hljs-params\">(p *reqPool)</span> <span class=\"hljs-title\">Put</span><span class=\"hljs-params\">(r request)</span></span> {\n\t<span class=\"hljs-keyword\">select</span> {\n\t<span class=\"hljs-keyword\">case</span> &lt;-r.respChan:\n\t<span class=\"hljs-keyword\">default</span>:\n\t}\n\tr.key = <span class=\"hljs-string\">&quot;&quot;</span>\n\tr.t = Invalid\n\t<span class=\"hljs-keyword\">select</span> {\n\t<span class=\"hljs-keyword\">case</span> p.c &lt;- r:\n\t}\n}\n</code></pre>\n<p>The above illustrates a simple channel implemented pool.\nThe first implementation was a sync.Pool.\nAfter further profiling however implementing our own proved to be easier on the heap.</p>\n<p>After plumbing the requst pool into the rest of the code pprof reports a much nicer result.</p>\n<pre class=\"hljs\"><code>(pprof) list \\.Lock\nTotal: 80.06MB\nROUTINE ======================== github.com/ldelossa/distlock.(*Manager).Lock in /home/louis/git/go/distlock/manager.go\n       1MB        1MB (flat, cum)  1.25% of Total\n         .          .     89:\t\treturn resp.ctx, func() {}\n         .          .     90:\t}\n         .          .     91:\n         .          .     92:\tm.propagateCancel(ctx, resp.ctx, key)\n         .          .     93:\n       1MB        1MB     94:\treturn resp.ctx, func() {\n         .          .     95:\t\tm.unlock(key)\n         .          .     96:\t}\n         .          .     97:}\n         .          .     98:\n         .          .     99:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {\n\n</code></pre>\n<h2>A PGX Trick</h2>\n<p>Removing the cost of the response-request model was a good win but there is still more to tune.</p>\n<p>Lets generate a graph of our call stack and associated allocations.</p>\n<pre class=\"hljs\"><code>‚ùØ go tool pprof -svg distlock.test memprofile.out\n</code></pre>\n<p><img src=\"/profile001.png\" alt=\"photo of high PGX allocations\"></p>\n<p>The above diagram is showing a large amount of allocations in PGX's getRows method.\nIts not rare for methods dealing with serialization to and from the database to allocate heavily.\nBut it would be nice if we could eliminate this.</p>\n<p>Getting a session pg advisory lock typically looks like this.</p>\n<pre class=\"hljs\"><code>SELECT pg_try_advisory_lock($1);\nSELECT pg_advisory_unlock($1);\n</code></pre>\n<p>Both lock functions return a table expression resulting in a true or a false.</p>\n<p>An optimization we can make is changing these queries to only return a row if the lock function returns true.\nOur application logic can then simply check whether any rows are returned and not read the contents.</p>\n<p>First lets fix our queries.</p>\n<pre class=\"hljs\"><code>SELECT lock FROM pg_try_advisory_lock($1) lock WHERE lock = true;\nSELECT lock FROM pg_advisory_unlock($1) lock WHERE lock = true;\n</code></pre>\n<p>A slight modification allows us to only return rows if the lock function returns true.</p>\n<p>The next step is to short circuit the PGX library from reading the rows.\nThis took a bit of library spelunking but I eventually discovered this...</p>\n<pre class=\"hljs\"><code>rr := m.conn.PgConn().ExecParams(ctx,\n    trySessionUnlock,\n    [][]<span class=\"hljs-keyword\">byte</span>{\n        keyify(key),\n    },\n    <span class=\"hljs-literal\">nil</span>,\n    []<span class=\"hljs-keyword\">int16</span>{<span class=\"hljs-number\">1</span>},\n    <span class=\"hljs-literal\">nil</span>)\ntag, err := rr.Close()\n<span class=\"hljs-keyword\">if</span> err != <span class=\"hljs-literal\">nil</span> {\n    <span class=\"hljs-keyword\">return</span> response{<span class=\"hljs-literal\">false</span>, <span class=\"hljs-literal\">nil</span>, err}\n}\n<span class=\"hljs-keyword\">if</span> tag.RowsAffected() == <span class=\"hljs-number\">0</span> {\n    <span class=\"hljs-keyword\">return</span> response{<span class=\"hljs-literal\">false</span>, <span class=\"hljs-literal\">nil</span>, fmt.Errorf(<span class=\"hljs-string\">&quot;unlock of key %s returned false&quot;</span>, key)}\n}\n</code></pre>\n<p>By using the lower level PgConn object we can exec our queries, get a response writer, and immediately close it to obtain the command tag.\nThe command tag tells us if any rows were affected by the exec. This effectively tells us whether the lock was obtained or not in a somewhat indirect way.</p>\n<p>Let's take a new 1 minute memory profile to see how this effects our heap.</p>\n<p><img src=\"/profile002.png\" alt=\"photo of high PGX allocations\"></p>\n<p>Notice the large improvement achieved.</p>\n<p>We can also compare the benchmark output.</p>\n<pre class=\"hljs\"><code>85149            890605 ns/op            1288 B/op         21 allocs/op\n</code></pre>\n<p>Where PGX was reading the rows.</p>\n<pre class=\"hljs\"><code>58051           1238353 ns/op             517 B/op         11 allocs/op\n</code></pre>\n<p>By eliminating the reading of rows we perform many more cycles and cut our allocation in roughly half.</p>\n<h2>Disclaimer on optimization</h2>\n<p>Is it worth to dig this deep into your allocations? Depends.\nIf you know the code you are writing will be in the &quot;hot-path&quot; its empowering to know what your allocation profile looks like.\nLearning the skills to performance tune is addicting and powerful but writing code that can be read and easily maintained should always be the first goal.\nThat being said I do think every engineer should go down the rabbit hole at least once. Its a lot of fun.</p>\n"}