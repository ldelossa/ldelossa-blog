<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <link href=https://meyerweb.com/eric/tools/css/reset/reset.css rel=stylesheet> <link href="https://fonts.googleapis.com/css2?family=Baloo+Paaji+2:wght@500&display=swap" rel=stylesheet> <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel=stylesheet> <link href="https://fonts.googleapis.com/css2?family=Muli&display=swap" rel=stylesheet> <link href=github-markdown.css rel=stylesheet> <link href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/zenburn.min.css rel=stylesheet> <script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js></script> <script src=//gc.zgo.at/count.js async data-goatcounter=https://ldelossa.goatcounter.com/count></script> <style>.layout.svelte-lybuj1{display:grid;height:100vh;width:100vw;grid-template-columns:1.5fr 6fr}.content-wrapper.svelte-lybuj1{grid-area:1/2/2/3;overflow-y:auto}.content-wrapper-full.svelte-lybuj1{grid-area:1/1/2/3}@media screen and (max-width:600px){.layout.svelte-lybuj1{display:grid;grid-template-columns:1fr}.content-wrapper.svelte-lybuj1{z-index:1}.content-wrapper-full.svelte-lybuj1{z-index:1}}.sidebar-open.svelte-posnvr{display:flex;justify-content:space-between;flex-direction:column;height:100vh;max-width:300px;min-width:275px;margin-right:1vmin;background:#4b6777;overflow-y:hidden}.sidebar-closed.svelte-posnvr{display:none}.sb-toggle-button-open.svelte-posnvr{z-index:100;outline:0;background-color:Transparent;border:none;padding:10px 12px;font-size:30px;cursor:pointer;color:#f3f8f2;font-family:Muli,cursive;position:fixed;left:0}.sb-toggle-button-open.svelte-posnvr:hover{color:#f2511b}.sb-toggle-button-closed.svelte-posnvr{z-index:100;outline:0;background-color:Transparent;border:none;padding:10px 12px;font-size:30px;cursor:pointer;color:#4b6777;font-family:Muli,cursive;position:fixed;left:0}.sb-toggle-button-closed.svelte-posnvr:hover{color:#f2511b}.author-wrapper.svelte-posnvr{display:flex;flex-direction:column;justify-content:center;margin:40px}.author.svelte-posnvr{text-align:center;font-size:2rem;color:#f3f8f2;font-family:Muli,sans-serif;letter-spacing:1px;text-shadow:2px 2px 0 rgba(0,0,0,.35),2px 2px 5px rgba(0,0,0,.5)}.console-wrapper.svelte-posnvr{display:flex;margin:20px;justify-content:center}nav.svelte-posnvr{display:flex;flex-direction:column;justify-content:space-around;flex-wrap:wrap}.icons-wrapper.svelte-posnvr{display:flex;flex-direction:column;justify-content:flex-end;padding:20px}@media screen and (max-width:600px){.sidebar-open.svelte-posnvr{-moz-box-shadow:initial;-webkit-box-shadow:initial;box-shadow:initial;max-width:initial;min-width:initial;margin-right:initial;border-right:initial;height:100vh;width:100vw}.icons-wrapper.svelte-posnvr{display:flex;grid-area:icons;flex-direction:column;justify-content:space-between}}pre.svelte-wc9ma6 code.svelte-wc9ma6{border-radius:3%;padding:8px;display:block;min-width:230px;max-width:275px;background:#333;color:#737373}.button-wrapper.svelte-n868dk{text-align:center;margin:1rem 0 1rem 0;padding:1rem 1rem 1rem 1rem}.button-ref.svelte-n868dk{text-decoration:underline;font-size:30px;color:#e9f1f7;font-family:Muli,sans-serif;letter-spacing:1px;text-shadow:2px 2px 0 rgba(0,0,0,.35),2px 2px 5px rgba(0,0,0,.5)}.button-ref.svelte-n868dk:hover{color:#f2511b;text-shadow:-1px -1px 0 rgba(242,81,27,.5),-1px -1px -5px rgba(242,81,27,.5)}.icons.svelte-13lfieo{display:flex;flex-direction:row;justify-content:space-around;flex-wrap:wrap}img.svelte-13lfieo{width:30px;height:30px}img.svelte-13lfieo:hover{-webkit-filter:drop-shadow(2px 2px 2px #222);filter:drop-shadow(2px 2px 2px #222)}.blog-wrapper.svelte-1ih628r{max-height:100vh;max-width:100vw}.markdown-body.svelte-1ih628r{min-width:100px;max-width:900px;margin:0 auto;padding:45px;color:#e9f1f7;font-family:Muli,sans-serif}@media(max-width:767px){.markdown-body.svelte-1ih628r{max-width:75%}}</style> <noscript id=sapper-head-start></noscript><title>Optimizing PGX Allocations in Golang with Pprof.</title><noscript id=sapper-head-end></noscript> <link href=/client/21d5f7fc0374dd879b15/main.js rel=preload as=script><link href=/client/21d5f7fc0374dd879b15/blog_$slug.2.js rel=preload as=script></head> <body> <div id=sapper> <div class="svelte-lybuj1 layout"><button class="svelte-posnvr sb-toggle-button-closed">☰</button> <div class="svelte-posnvr sidebar-closed"><div class="svelte-posnvr author-wrapper"><p class="svelte-posnvr author">Louis DeLosSantos<p><div class="svelte-posnvr console-wrapper"><pre class=svelte-wc9ma6><code class=svelte-wc9ma6>❯ tree ./ldelossa 
├── hardware
├── linux
├── music
└── software
❯ <span id=cursor>▊</span></code></pre></div></div> <nav class=svelte-posnvr><div class="svelte-n868dk button-wrapper"><a href=about class="svelte-n868dk button-ref">about</a></div> <div class="svelte-n868dk button-wrapper"><a href=resume class="svelte-n868dk button-ref">resume</a></div> <div class="svelte-n868dk button-wrapper"><a href=projects class="svelte-n868dk button-ref">projects</a></div> <div class="svelte-n868dk button-wrapper"><a href=blog class="svelte-n868dk button-ref">blog</a></div></nav> <div class="svelte-posnvr icons-wrapper"><div class="svelte-13lfieo icons"><a href=https://www.linkedin.com/in/louisdelossantos/ target=_blank><img alt="linkedin icon" src=linkedin.svg class=svelte-13lfieo></a> <a href=https://github.com/ldelossa target=_blank><img alt="github icon" src=github.svg class=svelte-13lfieo></a> <a href=https://twitter.com/ldelossa_ld target=_blank><img alt="twitter icon" src=twitter.svg class=svelte-13lfieo></a></div></div></div> <div class="svelte-lybuj1 content-wrapper-full"> <div class="svelte-1ih628r blog-wrapper"><article class="svelte-1ih628r markdown-body"><h1>Optimizing PGX Allocations in Golang with Pprof.</h1> <p>Performance tuning is one of those programming rituals that gets oddly addicting. Seems like humans have a fundamental impulse to make a graph plot in their desired direction. This can be seen in a wide assortment of fields. Day traders watch metrics focused on their net earnings, nutritionists keep their calorie counts logged, and programmers focusing on performance obsess over memory allocations.</p> <p>After spending sometime obessing myself I found myself making large allocation improvements with some tricks in the popular <a href=https://github.com/jackc/pgx>PGX</a> library.</p> <p>I'd like to shout out <em>Kale Blanekship</em> and <em>Eric Chlebek</em> from the performance channel in #gophers slack. They provided the clues used in this post.</p> <h2>The code</h2> <p>The code that's being profiled is a new distributed lock implementation for <a href=https://github.com/quay/claircore/ >ClairCore</a>. Postgres is the only required infrastructure for ClairCore by design. While it's not the best mechanim for a distributed lock, <a href=https://www.postgresql.org/docs/9.1/explicit-locking.html>postgres advisory locks</a> can be utilized to get <em>mostly</em> there.</p> <p>You can view the distlock implementation <a href=https://github.com/ldelossa/distlock>here</a></p> <h2>Reducing channel allocations</h2> <p>Our distlock implementation utilizes the request/response channel-of-channel pattern. A request object with a response channel is pushed onto a request channel. When the receiver gets the request it writes to the response channel, unblocking any client listening.</p> <p>This pattern is useful but will also alloc a lot of channels resulting in bloating the heap.</p> <p>To demonstrate this a benchmark will be taken that profiles lock acquisition and lock return.</p> <pre class=hljs><code><span class=hljs-meta>$</span><span class=bash> go <span class=hljs-built_in>test</span> -benchtime <span class=hljs-string>"1m"</span>  -run xxx -bench . -memprofile memprofile.out -cpuprofile cpuprofile.out</span>
</code></pre> <p>The command above runs a 1 minute benchmark profiling both memory and cpu.</p> <p>Next lets start an interactive pprof session over the memory profile and drill into the function where the channel allocations are occuring.</p> <pre class=hljs><code><span class=hljs-meta>$</span><span class=bash> go tool pprof distlock.test memprofile.out</span>

(pprof) list \.Lock
Total: 194.36MB
ROUTINE ======================== github.com/ldelossa/distlock.(*Manager).Lock in /home/louis/git/go/distlock/manager.go
      20MB       20MB (flat, cum) 10.29% of Total
         .          .     78:	}
         .          .     79:
         .          .     80:	req := request{
         .          .     81:		t:        Lock,
         .          .     82:		key:      key,
   13.50MB    13.50MB     83:		respChan: make(chan response),
         .          .     84:	}
         .          .     85:
         .          .     86:	// guaranteed to return
         .          .     87:	resp := m.g.request(req)
         .          .     88:
         .          .     89:	if !resp.ok {
         .          .     90:		return resp.ctx, func() {}
         .          .     91:	}
         .          .     92:
         .          .     93:	m.propagateCancel(ctx, resp.ctx, key)
         .          .     94:
    6.50MB     6.50MB     95:	return resp.ctx, func() {
         .          .     96:		m.unlock(key)
         .          .     97:	}
         .          .     98:}
         .          .     99:
         .          .    100:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {
</code></pre> <p>Above illustrates 13.50MB of heap memory is spent on allocating request objects and their response channels.</p> <p>We can introduce an object pool to promote the reuse of these channels.</p> <pre class=hljs><code><span class=hljs-keyword>type</span> reqPool <span class=hljs-keyword>struct</span> {
	c <span class=hljs-keyword>chan</span> request
}

<span class=hljs-function><span class=hljs-keyword>func</span> <span class=hljs-title>NewReqPool</span><span class=hljs-params>(seed <span class=hljs-keyword>int</span>)</span> *<span class=hljs-title>reqPool</span></span> {
	c := <span class=hljs-built_in>make</span>(<span class=hljs-keyword>chan</span> request, seed*<span class=hljs-number>2</span>)
	<span class=hljs-keyword>for</span> i := <span class=hljs-number>0</span>; i &lt; seed; i++ {
		r := request{respChan: <span class=hljs-built_in>make</span>(<span class=hljs-keyword>chan</span> response)}
		<span class=hljs-keyword>select</span> {
		<span class=hljs-keyword>case</span> c &lt;- r:
		<span class=hljs-keyword>default</span>:

		}
	}
	<span class=hljs-keyword>return</span> &reqPool{c}
}

<span class=hljs-function><span class=hljs-keyword>func</span> <span class=hljs-params>(p *reqPool)</span> <span class=hljs-title>Get</span><span class=hljs-params>()</span> <span class=hljs-title>request</span></span> {
	<span class=hljs-keyword>select</span> {
	<span class=hljs-keyword>case</span> r := &lt;-p.c:
		<span class=hljs-keyword>return</span> r
	<span class=hljs-keyword>default</span>:
		<span class=hljs-keyword>return</span> request{respChan: <span class=hljs-built_in>make</span>(<span class=hljs-keyword>chan</span> response)}
	}
}

<span class=hljs-function><span class=hljs-keyword>func</span> <span class=hljs-params>(p *reqPool)</span> <span class=hljs-title>Put</span><span class=hljs-params>(r request)</span></span> {
	<span class=hljs-keyword>select</span> {
	<span class=hljs-keyword>case</span> &lt;-r.respChan:
	<span class=hljs-keyword>default</span>:
	}
	r.key = <span class=hljs-string>""</span>
	r.t = Invalid
	<span class=hljs-keyword>select</span> {
	<span class=hljs-keyword>case</span> p.c &lt;- r:
	}
}
</code></pre> <p>The above illustrates a simple channel implemented pool. The first implementation was a sync.Pool. After further profiling however implementing our own proved to be easier on the heap.</p> <p>After plumbing the requst pool into the rest of the code pprof reports a much nicer result.</p> <pre class=hljs><code>(pprof) list \.Lock
Total: 80.06MB
ROUTINE ======================== github.com/ldelossa/distlock.(*Manager).Lock in /home/louis/git/go/distlock/manager.go
       1MB        1MB (flat, cum)  1.25% of Total
         .          .     89:		return resp.ctx, func() {}
         .          .     90:	}
         .          .     91:
         .          .     92:	m.propagateCancel(ctx, resp.ctx, key)
         .          .     93:
       1MB        1MB     94:	return resp.ctx, func() {
         .          .     95:		m.unlock(key)
         .          .     96:	}
         .          .     97:}
         .          .     98:
         .          .     99:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {

</code></pre> <h2>A PGX Trick</h2> <p>Removing the cost of the response-request model was a good win but there is still more to tune.</p> <p>Lets generate a graph of our call stack and associated allocations.</p> <pre class=hljs><code>❯ go tool pprof -svg distlock.test memprofile.out
</code></pre> <p><img alt="photo of high PGX allocations" src=/profile001.png></p> <p>The above diagram is showing a large amount of allocations in PGX's getRows method. Its not rare for methods dealing with serialization to and from the database to allocate heavily. But it would be nice if we could eliminate this.</p> <p>Getting a session pg advisory lock typically looks like this.</p> <pre class=hljs><code>SELECT pg_try_advisory_lock($1);
SELECT pg_advisory_unlock($1);
</code></pre> <p>Both lock functions return a table expression resulting in a true or a false.</p> <p>An optimization we can make is changing these queries to only return a row if the lock function returns true. Our application logic can then simply check whether any rows are returned and not read the contents.</p> <p>First lets fix our queries.</p> <pre class=hljs><code>SELECT lock FROM pg_try_advisory_lock($1) lock WHERE lock = true;
SELECT lock FROM pg_advisory_unlock($1) lock WHERE lock = true;
</code></pre> <p>A slight modification allows us to only return rows if the lock function returns true.</p> <p>The next step is to short circuit the PGX library from reading the rows. This took a bit of library spelunking but I eventually discovered this...</p> <pre class=hljs><code>rr := m.conn.PgConn().ExecParams(ctx,
    trySessionUnlock,
    [][]<span class=hljs-keyword>byte</span>{
        keyify(key),
    },
    <span class=hljs-literal>nil</span>,
    []<span class=hljs-keyword>int16</span>{<span class=hljs-number>1</span>},
    <span class=hljs-literal>nil</span>)
tag, err := rr.Close()
<span class=hljs-keyword>if</span> err != <span class=hljs-literal>nil</span> {
    <span class=hljs-keyword>return</span> response{<span class=hljs-literal>false</span>, <span class=hljs-literal>nil</span>, err}
}
<span class=hljs-keyword>if</span> tag.RowsAffected() == <span class=hljs-number>0</span> {
    <span class=hljs-keyword>return</span> response{<span class=hljs-literal>false</span>, <span class=hljs-literal>nil</span>, fmt.Errorf(<span class=hljs-string>"unlock of key %s returned false"</span>, key)}
}
</code></pre> <p>By using the lower level PgConn object we can exec our queries, get a response writer, and immediately close it to obtain the command tag. The command tag tells us if any rows were affected by the exec. This effectively tells us whether the lock was obtained or not in a somewhat indirect way.</p> <p>Let's take a new 1 minute memory profile to see how this effects our heap.</p> <p><img alt="photo of high PGX allocations" src=/profile002.png></p> <p>Notice the large improvement achieved.</p> <p>We can also compare the benchmark output.</p> <pre class=hljs><code>85149            890605 ns/op            1288 B/op         21 allocs/op
</code></pre> <p>Where PGX was reading the rows.</p> <pre class=hljs><code>58051           1238353 ns/op             517 B/op         11 allocs/op
</code></pre> <p>By eliminating the reading of rows we perform many more cycles and cut our allocation in roughly half.</p> <h2>Disclaimer on optimization</h2> <p>Is it worth to dig this deep into your allocations? Depends. If you know the code you are writing will be in the "hot-path" its empowering to know what your allocation profile looks like. Learning the skills to performance tune is addicting and powerful but writing code that can be read and easily maintained should always be the first goal. That being said I do think every engineer should go down the rabbit hole at least once. Its a lot of fun.</p> </article></div></div></div></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{article:{date:"2020-9-15",title:"Optimizing PGX Allocations in Golang with Pprof.",desc:"An example utilizing pprof for some impressive allocation reductions in the PGX sql library.",file:"allocation_optimization_in_go",html:"\u003Ch1\u003EOptimizing PGX Allocations in Golang with Pprof.\u003C\u002Fh1\u003E\n\u003Cp\u003EPerformance tuning is one of those programming rituals that gets oddly addicting.\nSeems like humans have a fundamental impulse to make a graph plot in their desired direction.\nThis can be seen in a wide assortment of fields.\nDay traders watch metrics focused on their net earnings, nutritionists keep their calorie counts logged, and programmers focusing on performance obsess over memory allocations.\u003C\u002Fp\u003E\n\u003Cp\u003EAfter spending sometime obessing myself I found myself making large allocation improvements with some tricks in the popular \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fjackc\u002Fpgx\"\u003EPGX\u003C\u002Fa\u003E library.\u003C\u002Fp\u003E\n\u003Cp\u003EI'd like to shout out \u003Cem\u003EKale Blanekship\u003C\u002Fem\u003E and \u003Cem\u003EEric Chlebek\u003C\u002Fem\u003E from the performance channel in #gophers slack. They provided the clues used in this post.\u003C\u002Fp\u003E\n\u003Ch2\u003EThe code\u003C\u002Fh2\u003E\n\u003Cp\u003EThe code that's being profiled is a new distributed lock implementation for \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fquay\u002Fclaircore\u002F\"\u003EClairCore\u003C\u002Fa\u003E.\nPostgres is the only required infrastructure for ClairCore by design.\nWhile it's not the best mechanim for a distributed lock, \u003Ca href=\"https:\u002F\u002Fwww.postgresql.org\u002Fdocs\u002F9.1\u002Fexplicit-locking.html\"\u003Epostgres advisory locks\u003C\u002Fa\u003E can be utilized to get \u003Cem\u003Emostly\u003C\u002Fem\u003E there.\u003C\u002Fp\u003E\n\u003Cp\u003EYou can view the distlock implementation \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fldelossa\u002Fdistlock\"\u003Ehere\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Ch2\u003EReducing channel allocations\u003C\u002Fh2\u003E\n\u003Cp\u003EOur distlock implementation utilizes the request\u002Fresponse channel-of-channel pattern.\nA request object with a response channel is pushed onto a request channel.\nWhen the receiver gets the request it writes to the response channel, unblocking any client listening.\u003C\u002Fp\u003E\n\u003Cp\u003EThis pattern is useful but will also alloc a lot of channels resulting in bloating the heap.\u003C\u002Fp\u003E\n\u003Cp\u003ETo demonstrate this a benchmark will be taken that profiles lock acquisition and lock return.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E\u003Cspan class=\"hljs-meta\"\u003E$\u003C\u002Fspan\u003E\u003Cspan class=\"bash\"\u003E go \u003Cspan class=\"hljs-built_in\"\u003Etest\u003C\u002Fspan\u003E -benchtime \u003Cspan class=\"hljs-string\"\u003E&quot;1m&quot;\u003C\u002Fspan\u003E  -run xxx -bench . -memprofile memprofile.out -cpuprofile cpuprofile.out\u003C\u002Fspan\u003E\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe command above runs a 1 minute benchmark profiling both memory and cpu.\u003C\u002Fp\u003E\n\u003Cp\u003ENext lets start an interactive pprof session over the memory profile and drill into the function where the channel allocations are occuring.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E\u003Cspan class=\"hljs-meta\"\u003E$\u003C\u002Fspan\u003E\u003Cspan class=\"bash\"\u003E go tool pprof distlock.test memprofile.out\u003C\u002Fspan\u003E\n\n(pprof) list \\.Lock\nTotal: 194.36MB\nROUTINE ======================== github.com\u002Fldelossa\u002Fdistlock.(*Manager).Lock in \u002Fhome\u002Flouis\u002Fgit\u002Fgo\u002Fdistlock\u002Fmanager.go\n      20MB       20MB (flat, cum) 10.29% of Total\n         .          .     78:\t}\n         .          .     79:\n         .          .     80:\treq := request{\n         .          .     81:\t\tt:        Lock,\n         .          .     82:\t\tkey:      key,\n   13.50MB    13.50MB     83:\t\trespChan: make(chan response),\n         .          .     84:\t}\n         .          .     85:\n         .          .     86:\t\u002F\u002F guaranteed to return\n         .          .     87:\tresp := m.g.request(req)\n         .          .     88:\n         .          .     89:\tif !resp.ok {\n         .          .     90:\t\treturn resp.ctx, func() {}\n         .          .     91:\t}\n         .          .     92:\n         .          .     93:\tm.propagateCancel(ctx, resp.ctx, key)\n         .          .     94:\n    6.50MB     6.50MB     95:\treturn resp.ctx, func() {\n         .          .     96:\t\tm.unlock(key)\n         .          .     97:\t}\n         .          .     98:}\n         .          .     99:\n         .          .    100:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EAbove illustrates 13.50MB of heap memory is spent on allocating request objects and their response channels.\u003C\u002Fp\u003E\n\u003Cp\u003EWe can introduce an object pool to promote the reuse of these channels.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E\u003Cspan class=\"hljs-keyword\"\u003Etype\u003C\u002Fspan\u003E reqPool \u003Cspan class=\"hljs-keyword\"\u003Estruct\u003C\u002Fspan\u003E {\n\tc \u003Cspan class=\"hljs-keyword\"\u003Echan\u003C\u002Fspan\u003E request\n}\n\n\u003Cspan class=\"hljs-function\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Efunc\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-title\"\u003ENewReqPool\u003C\u002Fspan\u003E\u003Cspan class=\"hljs-params\"\u003E(seed \u003Cspan class=\"hljs-keyword\"\u003Eint\u003C\u002Fspan\u003E)\u003C\u002Fspan\u003E *\u003Cspan class=\"hljs-title\"\u003EreqPool\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E {\n\tc := \u003Cspan class=\"hljs-built_in\"\u003Emake\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-keyword\"\u003Echan\u003C\u002Fspan\u003E request, seed*\u003Cspan class=\"hljs-number\"\u003E2\u003C\u002Fspan\u003E)\n\t\u003Cspan class=\"hljs-keyword\"\u003Efor\u003C\u002Fspan\u003E i := \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E; i &lt; seed; i++ {\n\t\tr := request{respChan: \u003Cspan class=\"hljs-built_in\"\u003Emake\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-keyword\"\u003Echan\u003C\u002Fspan\u003E response)}\n\t\t\u003Cspan class=\"hljs-keyword\"\u003Eselect\u003C\u002Fspan\u003E {\n\t\t\u003Cspan class=\"hljs-keyword\"\u003Ecase\u003C\u002Fspan\u003E c &lt;- r:\n\t\t\u003Cspan class=\"hljs-keyword\"\u003Edefault\u003C\u002Fspan\u003E:\n\n\t\t}\n\t}\n\t\u003Cspan class=\"hljs-keyword\"\u003Ereturn\u003C\u002Fspan\u003E &amp;reqPool{c}\n}\n\n\u003Cspan class=\"hljs-function\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Efunc\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-params\"\u003E(p *reqPool)\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-title\"\u003EGet\u003C\u002Fspan\u003E\u003Cspan class=\"hljs-params\"\u003E()\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-title\"\u003Erequest\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E {\n\t\u003Cspan class=\"hljs-keyword\"\u003Eselect\u003C\u002Fspan\u003E {\n\t\u003Cspan class=\"hljs-keyword\"\u003Ecase\u003C\u002Fspan\u003E r := &lt;-p.c:\n\t\t\u003Cspan class=\"hljs-keyword\"\u003Ereturn\u003C\u002Fspan\u003E r\n\t\u003Cspan class=\"hljs-keyword\"\u003Edefault\u003C\u002Fspan\u003E:\n\t\t\u003Cspan class=\"hljs-keyword\"\u003Ereturn\u003C\u002Fspan\u003E request{respChan: \u003Cspan class=\"hljs-built_in\"\u003Emake\u003C\u002Fspan\u003E(\u003Cspan class=\"hljs-keyword\"\u003Echan\u003C\u002Fspan\u003E response)}\n\t}\n}\n\n\u003Cspan class=\"hljs-function\"\u003E\u003Cspan class=\"hljs-keyword\"\u003Efunc\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-params\"\u003E(p *reqPool)\u003C\u002Fspan\u003E \u003Cspan class=\"hljs-title\"\u003EPut\u003C\u002Fspan\u003E\u003Cspan class=\"hljs-params\"\u003E(r request)\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E {\n\t\u003Cspan class=\"hljs-keyword\"\u003Eselect\u003C\u002Fspan\u003E {\n\t\u003Cspan class=\"hljs-keyword\"\u003Ecase\u003C\u002Fspan\u003E &lt;-r.respChan:\n\t\u003Cspan class=\"hljs-keyword\"\u003Edefault\u003C\u002Fspan\u003E:\n\t}\n\tr.key = \u003Cspan class=\"hljs-string\"\u003E&quot;&quot;\u003C\u002Fspan\u003E\n\tr.t = Invalid\n\t\u003Cspan class=\"hljs-keyword\"\u003Eselect\u003C\u002Fspan\u003E {\n\t\u003Cspan class=\"hljs-keyword\"\u003Ecase\u003C\u002Fspan\u003E p.c &lt;- r:\n\t}\n}\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EThe above illustrates a simple channel implemented pool.\nThe first implementation was a sync.Pool.\nAfter further profiling however implementing our own proved to be easier on the heap.\u003C\u002Fp\u003E\n\u003Cp\u003EAfter plumbing the requst pool into the rest of the code pprof reports a much nicer result.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E(pprof) list \\.Lock\nTotal: 80.06MB\nROUTINE ======================== github.com\u002Fldelossa\u002Fdistlock.(*Manager).Lock in \u002Fhome\u002Flouis\u002Fgit\u002Fgo\u002Fdistlock\u002Fmanager.go\n       1MB        1MB (flat, cum)  1.25% of Total\n         .          .     89:\t\treturn resp.ctx, func() {}\n         .          .     90:\t}\n         .          .     91:\n         .          .     92:\tm.propagateCancel(ctx, resp.ctx, key)\n         .          .     93:\n       1MB        1MB     94:\treturn resp.ctx, func() {\n         .          .     95:\t\tm.unlock(key)\n         .          .     96:\t}\n         .          .     97:}\n         .          .     98:\n         .          .     99:func (m *Manager) propagateCancel(parent context.Context, child context.Context, key string) {\n\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Ch2\u003EA PGX Trick\u003C\u002Fh2\u003E\n\u003Cp\u003ERemoving the cost of the response-request model was a good win but there is still more to tune.\u003C\u002Fp\u003E\n\u003Cp\u003ELets generate a graph of our call stack and associated allocations.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E❯ go tool pprof -svg distlock.test memprofile.out\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fprofile001.png\" alt=\"photo of high PGX allocations\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EThe above diagram is showing a large amount of allocations in PGX's getRows method.\nIts not rare for methods dealing with serialization to and from the database to allocate heavily.\nBut it would be nice if we could eliminate this.\u003C\u002Fp\u003E\n\u003Cp\u003EGetting a session pg advisory lock typically looks like this.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003ESELECT pg_try_advisory_lock($1);\nSELECT pg_advisory_unlock($1);\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EBoth lock functions return a table expression resulting in a true or a false.\u003C\u002Fp\u003E\n\u003Cp\u003EAn optimization we can make is changing these queries to only return a row if the lock function returns true.\nOur application logic can then simply check whether any rows are returned and not read the contents.\u003C\u002Fp\u003E\n\u003Cp\u003EFirst lets fix our queries.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003ESELECT lock FROM pg_try_advisory_lock($1) lock WHERE lock = true;\nSELECT lock FROM pg_advisory_unlock($1) lock WHERE lock = true;\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EA slight modification allows us to only return rows if the lock function returns true.\u003C\u002Fp\u003E\n\u003Cp\u003EThe next step is to short circuit the PGX library from reading the rows.\nThis took a bit of library spelunking but I eventually discovered this...\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003Err := m.conn.PgConn().ExecParams(ctx,\n    trySessionUnlock,\n    [][]\u003Cspan class=\"hljs-keyword\"\u003Ebyte\u003C\u002Fspan\u003E{\n        keyify(key),\n    },\n    \u003Cspan class=\"hljs-literal\"\u003Enil\u003C\u002Fspan\u003E,\n    []\u003Cspan class=\"hljs-keyword\"\u003Eint16\u003C\u002Fspan\u003E{\u003Cspan class=\"hljs-number\"\u003E1\u003C\u002Fspan\u003E},\n    \u003Cspan class=\"hljs-literal\"\u003Enil\u003C\u002Fspan\u003E)\ntag, err := rr.Close()\n\u003Cspan class=\"hljs-keyword\"\u003Eif\u003C\u002Fspan\u003E err != \u003Cspan class=\"hljs-literal\"\u003Enil\u003C\u002Fspan\u003E {\n    \u003Cspan class=\"hljs-keyword\"\u003Ereturn\u003C\u002Fspan\u003E response{\u003Cspan class=\"hljs-literal\"\u003Efalse\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-literal\"\u003Enil\u003C\u002Fspan\u003E, err}\n}\n\u003Cspan class=\"hljs-keyword\"\u003Eif\u003C\u002Fspan\u003E tag.RowsAffected() == \u003Cspan class=\"hljs-number\"\u003E0\u003C\u002Fspan\u003E {\n    \u003Cspan class=\"hljs-keyword\"\u003Ereturn\u003C\u002Fspan\u003E response{\u003Cspan class=\"hljs-literal\"\u003Efalse\u003C\u002Fspan\u003E, \u003Cspan class=\"hljs-literal\"\u003Enil\u003C\u002Fspan\u003E, fmt.Errorf(\u003Cspan class=\"hljs-string\"\u003E&quot;unlock of key %s returned false&quot;\u003C\u002Fspan\u003E, key)}\n}\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EBy using the lower level PgConn object we can exec our queries, get a response writer, and immediately close it to obtain the command tag.\nThe command tag tells us if any rows were affected by the exec. This effectively tells us whether the lock was obtained or not in a somewhat indirect way.\u003C\u002Fp\u003E\n\u003Cp\u003ELet's take a new 1 minute memory profile to see how this effects our heap.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"\u002Fprofile002.png\" alt=\"photo of high PGX allocations\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ENotice the large improvement achieved.\u003C\u002Fp\u003E\n\u003Cp\u003EWe can also compare the benchmark output.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E85149            890605 ns\u002Fop            1288 B\u002Fop         21 allocs\u002Fop\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EWhere PGX was reading the rows.\u003C\u002Fp\u003E\n\u003Cpre class=\"hljs\"\u003E\u003Ccode\u003E58051           1238353 ns\u002Fop             517 B\u002Fop         11 allocs\u002Fop\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cp\u003EBy eliminating the reading of rows we perform many more cycles and cut our allocation in roughly half.\u003C\u002Fp\u003E\n\u003Ch2\u003EDisclaimer on optimization\u003C\u002Fh2\u003E\n\u003Cp\u003EIs it worth to dig this deep into your allocations? Depends.\nIf you know the code you are writing will be in the &quot;hot-path&quot; its empowering to know what your allocation profile looks like.\nLearning the skills to performance tune is addicting and powerful but writing code that can be read and easily maintained should always be the first goal.\nThat being said I do think every engineer should go down the rabbit hole at least once. Its a lot of fun.\u003C\u002Fp\u003E\n"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');</script><script src=/client/21d5f7fc0374dd879b15/main.js></script> 